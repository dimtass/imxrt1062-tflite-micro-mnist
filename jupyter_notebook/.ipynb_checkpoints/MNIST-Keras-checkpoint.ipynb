{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST keras model\n",
    "\n",
    "This notebook is part of this [post](https://www.stupid-projects.com/machine-learning-on-embedded-part-1) which is part a series of post about using ML and NN in embedded MCUs.\n",
    "\n",
    "I've taken this notebook has been taken from this github repo and just added a few stuff:\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "## Additions\n",
    "You don't have to use all the notebook. The first part is only if you want to train again your model. Currently the trained model is already part of the stm32f746 firmware. In the last part I've added a small part of code that you can draw a digit and then send it to the stm32f746 and get back the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.1 - Introduction to convnets\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been \n",
    "through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its \n",
    "accuracy will still blow out of the water that of the densely-connected model from Chapter 2.\n",
    "\n",
    "The 6 lines of code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a \n",
    "minute what they do concretely.\n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n",
    "In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n",
    "passing the argument `input_shape=(28, 28, 1)` to our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n",
    "and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n",
    "the `Conv2D` layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n",
    "already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n",
    "So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n",
    "looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n",
    "\n",
    "Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter \n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # Quantization aware training\n",
    "# sess = tf.keras.backend.get_session()\n",
    "# tf.contrib.quantize.create_training_graph(sess.graph)\n",
    "# sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 21:56:08.309328 140468425586496 deprecation_wrapper.py:119] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 21:56:08.324759 140468425586496 deprecation_wrapper.py:119] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 21:56:08.384036 140468425586496 deprecation.py:323] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 21:56:08.445558 140468425586496 deprecation_wrapper.py:119] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1706 - acc: 0.9455\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0470 - acc: 0.9854\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0320 - acc: 0.9899\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0234 - acc: 0.9927\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0192 - acc: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1002d7630>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we \n",
    "decreased our error rate by 68% (relative). Not bad! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = 'mnist-quant.h5'\n",
    "model.save(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Keras model to tflite\n",
    "\n",
    "Now we can use the TFLiteConverter to convert the Keras model to the\n",
    "tflite flatbuffer. When you run the converter then the number you'll\n",
    "get as response is the size of the tflite model. In this example we\n",
    "use the `tf.lite.Optimize.OPTIMIZE_FOR_SIZE` optimization to reduce\n",
    "the size. Have in mind that when reduce the size then the output is\n",
    "not `uint8` but `int8`, which currently is not supported in tflite-micro.\n",
    "Therefore, in case you want to convert the model for that use you need\n",
    "to comment out that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.14.0-rc1-22-gaf24dc9 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0714 21:57:38.455404 140468425586496 deprecation.py:323] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0714 21:57:38.456239 140468425586496 deprecation.py:323] From /home/dimtass/miniconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375740"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model = 'mnist-quant.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model)\n",
    "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model_cnv = converter.convert()\n",
    "open(tflite_model, 'wb').write(tflite_model_cnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a header file from the tflite model\n",
    "Now that you have your tflite flatbuffer you can convert it to a header file\n",
    "in order to add it to your C++ code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'' None\n"
     ]
    }
   ],
   "source": [
    "bashCommand = 'xxd -i mnist-quant.tflite > model_data.h'\n",
    "import subprocess\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "print(output, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model for X-CUBE-AI\n",
    "\n",
    "Now we save the model and will import the model with the x-cube-ai plugin in CubeMX.\n",
    "The h5 file contains information about the model and also the calculated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate the model\n",
    "\n",
    "In order to make it more interesting, I've wrote a small python that you can draw a digit with your mouse and then ran the prediction function to evaluate the result. For this purpose I'm using tkinter and the PIL library. Therefore you need to install them in your environment.\n",
    "\n",
    "For ubuntu (I'm using conda):\n",
    "```sh\n",
    "sudo apt install python3-tk\n",
    "conda install Pillow\n",
    "```\n",
    "\n",
    "#### How to use:\n",
    "Run the following two cells and this window will show up.\n",
    "\n",
    "![Image](./stm32comms/digit_draw_1.png)\n",
    "\n",
    "In the left window you can draw any digit with your mouse by clicking in the white area. Just be sure that you don't draw that too fast because then you get dotted lines. Then you can either press one of the following buttons:\n",
    "* `Clear`: clears the input drawing area\n",
    "* `Export`: Converts the draw digit to the MNIST input format and then exports the digit to a file called `digit.txt`. You can use this file in this notebook and evaluate the result.\n",
    "* `Inference`: Converts the digit to the MNIST input format and sends the data to the MCU via the serial port. Then the MCU runs the prediction and returns an array with the output values and the time that spend for the calculation.\n",
    "\n",
    "This is an example (I'm right-handed but I'm the mouse with my left hand, this is why it seems so ugly, lol).\n",
    "\n",
    "![Image](./stm32comms/digit_draw_2.png)\n",
    "\n",
    "\n",
    "Anyway, try yourself by running the next two cells.\n",
    "\n",
    "> Warning: If you proceed with the export function and local evaluation, then you need first to terminate the tkinter window, because the notebook is not able to run 2 cells at the same time. Therefore, if the window thread is running then no other cell can be run.\n",
    "\n",
    "> Note: Be carefull that if you do any changes in any python class or script that is already loaded from the jupyter notebook kernel, then you need to restart the kernel (File menu: Kernel-> Restart). Otherwise the previous loaded class will be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *\n",
    "from stm32comms.MnistDigitDraw import MnistDigitDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported image to digit.txt\n"
     ]
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.title(\"MNIST digit draw\")\n",
    "d = MnistDigitDraw(root, 250, 250)\n",
    "d.start()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate hand-written digit on the notebook\n",
    "\n",
    "In case you wnt to test your hand-written digit here then run the following cells. The next cells will load the `digit.txt` file which was exported in the previous step.\n",
    "\n",
    "The imported digit share is (768,). If you just open the file in a text editor (or cat the file) you'll see that each pixel value is one line. Therefore, after loading the digit then you need to convert from `(768,)` to `(1, 28, 28, 1)`, which is the format that the model prediction function support. To do that you need first to reshape the array to `(28, 28)` and then add two additional dimension in the tensor, one dimension in the beginning and one in the end. Those don't need to have a value, the first dimension is used as an index and the last as the prediction result, but it this case we don't care for any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load digit\n",
    "digit = np.loadtxt('digit.txt')\n",
    "# Reshape\n",
    "x = digit.reshape(28,28)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = np.expand_dims(x, axis=3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.43502294e-08, 3.31315153e-09, 3.03140750e-05, 9.97137977e-04,\n",
       "        2.41599232e-12, 1.34075648e-07, 5.32690038e-11, 4.32082814e-08,\n",
       "        9.98972297e-01, 1.16058033e-07]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result the one which has the larger value is the predicted digit.\n",
    "\n",
    "#### Print imorted digit\n",
    "Just for visual inspection you can also print the imported digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
