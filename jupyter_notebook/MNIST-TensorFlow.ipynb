{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST for Tensorflow-lite\n",
    "----\n",
    "\n",
    "This notebook is part of this [post](https://www.stupid-projects.com/tensorflow-2-1-0-for-microcontrollers-benchmarks-on-teensy-4-0/) which is part a series of post about using ML and NN in embedded MCUs. The first post of the series is [here](https://www.stupid-projects.com/machine-learning-on-embedded-part-1)\n",
    "\n",
    "This notebook is just a port of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook from Keras to TF.\n",
    "\n",
    "This notebook is meant to be used to train the MNIST NN and then export the model to TF-Lite for microcontrollers and uploaded to a Teensy 4.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the model\n",
    "\n",
    "As it's mentioned before, this is just a port from Keras to TF of [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb) notebook. For the model training we're going to use `convnets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.1.0\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train and test data\n",
    "Normally when the dataset is loaded the shape is (x, 28, 28). For convnets you need to reshape the data to (x, 28, 28, y), where `x` is the number of images per set and `y` in this case is the number of colors. Normally, of RGB it should be 3, but since the images are grayscale then it's 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.ndarray'>\n",
      "Dataset shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type:\", type(train_images))\n",
    "print(\"Dataset shape:\", (train_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 60000\n",
      "Possible values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", len(train_labels))\n",
    "print(\"Possible values:\", np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a digit from the dataset\n",
    "Now we just print a digit from the dataset in order to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.figure()\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Classification label: {}\".format(train_labels[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.1789 - accuracy: 0.9437\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0468 - accuracy: 0.9850\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0325 - accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0248 - accuracy: 0.9922\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0198 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f156c0a2828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0305 - accuracy: 0.9917\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0305 - accuracy: 0.9917\n",
      "Restored model, accuracy: 99.17%\n",
      "Restored model, loss: 0.030485744241706015\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "print(\"Restored model, loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the model to tflite\n",
    "\n",
    "Now we need to export the model and save it in a `h5` file. Then we use the `TFLiteConverter` to convert the model to the flatbuffer tflite format.\n",
    "\n",
    "Normally, we should use quantization on the model as it's explained [here](https://www.tensorflow.org/lite/microcontrollers/build_convert#quantization), but for some reason in the current version I'm using (1.14) that doesn't work and when the model is loaded on the stm32f746, then I get this error:\n",
    "\n",
    "```\n",
    "Only float32, int16, int32, int64, uint8, bool, complex64 supported currently\n",
    "```\n",
    "\n",
    "This error comes from the `source/libs/tensorflow/lite/experimental/micro/simple_tensor_allocator.cc` file and the reason is that when the model is converted with `TFLiteConverter`, then the output is set to `kTfLiteInt8`, which means signed integer and that is not yet supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist-tflite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you want to add post-quantization during conversion (which doesn't work yet), then you need to uncomment the line in the next code. Finally, the output of the next command is the size of the flatbuffer model in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the converted flatbuffer is: 376060 bytes\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "  for x in x_test_normalized:\n",
    "    yield [np.array([x], dtype=np.float32)]\n",
    "    \n",
    "tflite_mnist_model = 'mnist.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "flatbuffer_size = open(tflite_mnist_model, \"wb\").write(tflite_model)\n",
    "\n",
    "print('The size of the converted flatbuffer is: %d bytes' % flatbuffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a header file from the tflite model\n",
    "Now that you have your tflite flatbuffer you can convert it to a header file\n",
    "in order to add it to your C++ code you need to run this command in bash in\n",
    "the `jupyter _notebook` folder.\n",
    "\n",
    "```sh\n",
    "xxd -i jupyter_notebook/mnist.tflite > source/src/inc/model_data.h\n",
    "```\n",
    "\n",
    "#### Note:\n",
    "In the `source/src/inc/model_data.h` you need to change this line:\n",
    "```cpp\n",
    "unsigned char jupyter_notebook_mnist_tflite[] = {\n",
    "```\n",
    "to this:\n",
    "```cpp\n",
    "const unsigned char jupyter_notebook_mnist_tflite[] = {\n",
    "```\n",
    "Otherwise it won't fit in the RAM and you'll get this error:\n",
    "```sh\n",
    "imxrt1062-mnist-tflite.elf section `.bss' will not fit in region `RAM'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load model and interpreter\n",
    "\n",
    "To evaluate the hand-written digit in the notebook then you need to create an interpreter and then feed the image (or array) in the input. You can create that here and use it later in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: conv2d_input\n",
      "shape: [ 1 28 28  1]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "name: Identity\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "DUMP INPUT\n",
      "{'name': 'conv2d_input', 'index': 1, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "\n",
      "DUMP OUTPUT\n",
      "{'name': 'Identity', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_mnist_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\nDUMP INPUT\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"\\nDUMP OUTPUT\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evalutate the model\n",
    "\n",
    "In order to make it more interesting, I've wrote a small python that you can draw a digit with your mouse and then ran the prediction function to evaluate the result. For this purpose I'm using tkinter and the PIL library. Therefore you need to install them in your environment.\n",
    "\n",
    "For ubuntu (I'm using conda):\n",
    "```sh\n",
    "sudo apt install python3-tk\n",
    "conda install Pillow\n",
    "```\n",
    "\n",
    "#### How to use:\n",
    "Run the following two cells and this window will show up.\n",
    "\n",
    "![Image](./MnistDigitDraw/digit_draw_1.png)\n",
    "\n",
    "In the left window you can draw any digit with your mouse by clicking in the white area. Just be sure that you don't draw that too fast because then you get dotted lines. Then you can either press one of the following buttons:\n",
    "* `Clear`: clears the input drawing area\n",
    "* `Export`: Converts the draw digit to the MNIST input format and then exports the digit to a file called `digit.txt`. You can use this file in this notebook and evaluate the result.\n",
    "* `Inference`: Converts the digit to the MNIST input format and sends the data to the MCU via the serial port. Then the MCU runs the prediction and returns an array with the output values and the time that spend for the calculation.\n",
    "\n",
    "This is an example (I'm right-handed but I'm the mouse with my left hand, this is why it seems so ugly, lol).\n",
    "\n",
    "![Image](./MnistDigitDraw/digit_draw_2.png)\n",
    "\n",
    "\n",
    "Anyway, try yourself by running the next two cells.\n",
    "\n",
    "> Warning: If you proceed with the export function and local evaluation, then you need first to terminate the tkinter window, because the notebook is not able to run 2 cells at the same time. Therefore, if the window thread is running then no other cell can be run.\n",
    "\n",
    "> Note: Be carefull that if you do any changes in any python class or script that is already loaded from the jupyter notebook kernel, then you need to restart the kernel (File menu: Kernel-> Restart). Otherwise the previous loaded class will be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tkinter import *\n",
    "from MnistDigitDraw.MnistDigitDraw import MnistDigitDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the followinf cell how many times you like in order to draw a new digit every time. When you do, then press the `Inference` button, then the `Export` and then close the window to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the TK window to draw the digit and save it in a file\n",
    "root = Tk()\n",
    "root.title(\"MNIST digit draw\")\n",
    "d = MnistDigitDraw(root, 250, 250)\n",
    "d.start()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digit from the file that created above\n",
    "digit = np.loadtxt('digit.txt')\n",
    "# Reshape\n",
    "loaded_digit = digit.reshape(28,28)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=0)\n",
    "loaded_digit = np.expand_dims(loaded_digit, axis=3)\n",
    "loaded_digit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the digit that you drew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(loaded_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate hand-written digit on the notebook\n",
    "\n",
    "In case you wnt to test your hand-written digit here then run the following cells. The next cells will load the `digit.txt` file which was exported in the previous step.\n",
    "\n",
    "The imported digit share is (768,). If you just open the file in a text editor (or cat the file) you'll see that each pixel value is one line. Therefore, after loading the digit then you need to convert from `(768,)` to `(1, 28, 28, 1)`, which is the format that the model prediction function support. To do that you need first to reshape the array to `(28, 28)` and then add two additional dimension in the tensor, one dimension in the beginning and one in the end. Those don't need to have a value, the first dimension is used as an index and the last as the prediction result, but it this case we don't care for any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results: [[1.5875762e-10 2.0665674e-08 1.0000000e+00 9.0883438e-12 3.8147090e-13\n",
      "  3.2608371e-15 1.3031773e-14 2.2391200e-09 7.7385898e-10 1.3262178e-12]]\n",
      "Predicted value: 2\n"
     ]
    }
   ],
   "source": [
    "loaded_digit = loaded_digit.astype('float32')\n",
    "input_details = interpreter.get_input_details()\n",
    "interpreter.set_tensor(input_details[0]['index'], loaded_digit)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results:\", output_data)\n",
    "print(\"Predicted value:\", np.argmax(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result the one which has the larger value is the predicted digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export hand-draw digit for use in the source code\n",
    "\n",
    "For testing and development you can export the digit you've drawn in a header file and then compile the code and upload it to the Tennsy 4.0. When you do that, then in order to save RAM you need to convert the array to `const`. The following script does that for you, though.\n",
    "\n",
    "> Note: Only run the next cell if you want to override the default digit in the flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes = digit.tobytes\n",
    "np.save(\"bytes.txt\", bytes)\n",
    "f= open(\"../source/src/inc/digit.h\",\"w+\")\n",
    "f.write(\"const float digit[] = { \\n\")\n",
    "for d in digit:\n",
    "    f.write(str(d))\n",
    "    f.write(',')\n",
    "f.write('\\n};')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build the code and flash target\n",
    "\n",
    "Now on your console run these commands to build and flash the project on the Teensy 4.0\n",
    "\n",
    "```sh\n",
    "./docker-build.sh \"USE_COMP_MODEL=OFF USE_CMSIS_NN=ON ./build.sh\"\n",
    "./flash.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
